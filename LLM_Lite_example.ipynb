{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8-NdxhbuKugB",
        "outputId": "612b0d81-efd0-4f57-f9eb-4782ce00f926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litellm==1.44.9\n",
            "  Downloading litellm-1.44.9-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (3.11.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (1.57.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (2.10.3)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm==1.44.9)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (2.32.3)\n",
            "Collecting tiktoken>=0.7.0 (from litellm==1.44.9)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9) (0.21.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm==1.44.9) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.44.9) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9) (0.22.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.44.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.44.9) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.7.0->litellm==1.44.9) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm==1.44.9) (0.27.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.40.0->litellm==1.44.9) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->litellm==1.44.9) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40.0->litellm==1.44.9) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.44.9) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.44.9) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.44.9) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.44.9) (6.0.2)\n",
            "Downloading litellm-1.44.9-py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, tiktoken, litellm\n",
            "Successfully installed litellm-1.44.9 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install 'litellm'==1.44.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n"
      ],
      "metadata": {
        "id": "Df-SEPDpLprk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion, acompletion\n",
        "from pprint import pprint\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = completion(\n",
        "        model=\"cohere/command-r-plus-08-2024\",\n",
        "        messages=messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=200\n",
        "    )"
      ],
      "metadata": {
        "id": "GZcicgChMEeA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jpLFd3b5NB0g",
        "outputId": "4542db57-d330-4199-feb4-04e34b8ba2d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'command-r-plus-08-2024'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "gpXDuBccND7v",
        "outputId": "d3e20e78-46ea-497b-f7c4-7d7d36471e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('تبدو السماء زرقاء خلال النهار بسبب ظاهرة تسمى تشتت رايلي. عندما تصل أشعة '\n",
            " 'الشمس إلى الغلاف الجوي للأرض، فإنها تصطدم بجزيئات الغلاف الجوي مثل '\n",
            " 'النيتروجين والأكسجين. هذه الجزيئات أصغر بكثير من أطوال موجات الضوء المرئي، '\n",
            " 'مما يؤدي إلى تشتت الضوء بجميع ألوان الطيف.\\n'\n",
            " '\\n'\n",
            " 'يتم تشتيت الضوء الأزرق بشكل أكبر من الألوان الأخرى بسبب طوله الموجي الأقصر. '\n",
            " 'عندما تنتشر جزيئات الغلاف الجوي الضوء الأزرق في جميع الاتجاهات، فإنه ينتشر '\n",
            " 'عبر السماء، مما يجعلها تبدو زرقاء للعين المجردة.\\n'\n",
            " '\\n'\n",
            " 'يتم تشتيت الألوان الأخرى أيضًا، ولكن بدرجة أقل من اللون الأزرق. على سبيل '\n",
            " 'المثال، يتم تشتيت الضوء الأحمر')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SDK Logging"
      ],
      "metadata": {
        "id": "qAoLNNX3PZLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "import os\n",
        "import json\n",
        "\n",
        "logs_dir = \"./llm-logs\"\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "def log_success(kwargs, completion_obj, start_time, end_time):\n",
        "    with open(f\"{logs_dir}/success-logs.jsonl\", \"a\") as dest:\n",
        "        dest.write(\n",
        "            json.dumps({\n",
        "                \"kwargs\": kwargs,\n",
        "                \"completion_obj\": completion_obj,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "            }, ensure_ascii=False, default=str ) + \"\\n\"\n",
        "        )\n",
        "\n",
        "def log_failure(kwargs, completion_obj, start_time, end_time):\n",
        "    with open(f\"{logs_dir}/failure-logs.jsonl\", \"a\") as dest:\n",
        "        dest.write(\n",
        "            json.dumps({\n",
        "                \"kwargs\": kwargs,\n",
        "                \"completion_obj\": completion_obj,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "            }, ensure_ascii=False, default=str ) + \"\\n\"\n",
        "        )\n",
        "\n",
        "litellm.success_callback = [log_success]\n",
        "litellm.failure_callback = [log_failure]"
      ],
      "metadata": {
        "id": "qpn2fWNBPbmB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = completion(\n",
        "            model=\"gemini/gemini-pro\",\n",
        "            messages=messages,\n",
        "            temperature=0.5,\n",
        "            max_tokens=200\n",
        "        )"
      ],
      "metadata": {
        "id": "KaymhSbuRTOx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proxy Server"
      ],
      "metadata": {
        "id": "ZsVOBXD9TE3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'litellm[proxy]'==1.44.9"
      ],
      "metadata": {
        "id": "p7dvX7RoTJV0",
        "collapsed": true,
        "outputId": "7f115193-71e4-476c-ebaf-be9c0360545f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: litellm==1.44.9 in /usr/local/lib/python3.10/dist-packages (from litellm[proxy]==1.44.9) (1.44.9)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (3.11.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (1.57.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (2.10.3)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (1.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (2.32.3)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (0.8.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm==1.44.9->litellm[proxy]==1.44.9) (0.21.0)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm[proxy]==1.44.9) (2.10.1)\n",
            "Collecting apscheduler<4.0.0,>=3.10.4 (from litellm[proxy]==1.44.9)\n",
            "  Downloading APScheduler-3.11.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting backoff (from litellm[proxy]==1.44.9)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting cryptography<43.0.0,>=42.0.5 (from litellm[proxy]==1.44.9)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting fastapi<0.112.0,>=0.111.0 (from litellm[proxy]==1.44.9)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting fastapi-sso<0.11.0,>=0.10.0 (from litellm[proxy]==1.44.9)\n",
            "  Downloading fastapi_sso-0.10.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting gunicorn<23.0.0,>=22.0.0 (from litellm[proxy]==1.44.9)\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from litellm[proxy]==1.44.9) (3.10.12)\n",
            "Collecting pynacl<2.0.0,>=1.5.0 (from litellm[proxy]==1.44.9)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting python-multipart<0.0.10,>=0.0.9 (from litellm[proxy]==1.44.9)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from litellm[proxy]==1.44.9) (6.0.2)\n",
            "Collecting rq (from litellm[proxy]==1.44.9)\n",
            "  Downloading rq-2.1.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting uvicorn<0.23.0,>=0.22.0 (from litellm[proxy]==1.44.9)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.10/dist-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]==1.44.9) (5.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.5->litellm[proxy]==1.44.9) (1.17.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (4.12.2)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (0.28.1)\n",
            "Collecting email_validator>=2.0.0 (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from fastapi-sso<0.11.0,>=0.10.0->litellm[proxy]==1.44.9) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn<23.0.0,>=22.0.0->litellm[proxy]==1.44.9) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm==1.44.9->litellm[proxy]==1.44.9) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.44.9->litellm[proxy]==1.44.9) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9->litellm[proxy]==1.44.9) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9->litellm[proxy]==1.44.9) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9->litellm[proxy]==1.44.9) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.44.9->litellm[proxy]==1.44.9) (0.22.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9->litellm[proxy]==1.44.9) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9->litellm[proxy]==1.44.9) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9->litellm[proxy]==1.44.9) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9->litellm[proxy]==1.44.9) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->litellm==1.44.9->litellm[proxy]==1.44.9) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.44.9->litellm[proxy]==1.44.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.44.9->litellm[proxy]==1.44.9) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9->litellm[proxy]==1.44.9) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9->litellm[proxy]==1.44.9) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9->litellm[proxy]==1.44.9) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm==1.44.9->litellm[proxy]==1.44.9) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.7.0->litellm==1.44.9->litellm[proxy]==1.44.9) (2024.11.6)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn<0.23.0,>=0.22.0->litellm[proxy]==1.44.9) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm==1.44.9->litellm[proxy]==1.44.9) (1.18.3)\n",
            "Collecting redis>=3.5 (from rq->litellm[proxy]==1.44.9)\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm==1.44.9->litellm[proxy]==1.44.9) (0.27.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.40.0->litellm==1.44.9->litellm[proxy]==1.44.9) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->litellm[proxy]==1.44.9) (2.22)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (0.15.1)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading rich_toolkit-0.12.0-py3-none-any.whl.metadata (966 bytes)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (1.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.44.9->litellm[proxy]==1.44.9) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.44.9->litellm[proxy]==1.44.9) (2024.10.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (14.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.44.9) (0.1.2)\n",
            "Downloading APScheduler-3.11.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_sso-0.10.0-py3-none-any.whl (16 kB)\n",
            "Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading rq-2.1.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.12.0-py3-none-any.whl (13 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvloop, uvicorn, redis, python-multipart, httptools, gunicorn, dnspython, backoff, apscheduler, watchfiles, starlette, rq, pynacl, email_validator, cryptography, rich-toolkit, fastapi-cli, fastapi, fastapi-sso\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "Successfully installed apscheduler-3.11.0 backoff-2.2.1 cryptography-42.0.8 dnspython-2.7.0 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.7 fastapi-sso-0.10.0 gunicorn-22.0.0 httptools-0.6.4 pynacl-1.5.0 python-multipart-0.0.9 redis-5.2.1 rich-toolkit-0.12.0 rq-2.1.0 starlette-0.37.2 uvicorn-0.22.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ check any litellm processes\n",
        "# !pgrep -fl litellm\n",
        "\n",
        "# ============ kill any litellm processes\n",
        "# !pkill -f litellm"
      ],
      "metadata": {
        "id": "_ifl3c6zWrWF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "UCeCZ_CSV-EG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm.yaml\n",
        "model_list:\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUbRqM7iTb_X",
        "outputId": "cf090924-cea2-4cc6-b2da-d67c42994dde"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "1300kJJYVMxU",
        "outputId": "46a86355-c2e6-48e0-de87-d98d8a628aec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "INFO:     Started server process [907]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "# from pprint import pprint\n",
        "\n",
        "# client = openai.OpenAI(\n",
        "#     api_key=\"any key\",\n",
        "#     base_url=\"http://0.0.0.0:4000\"\n",
        "# )"
      ],
      "metadata": {
        "id": "vKMgq4qWW_un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# response = client.chat.completions.create(\n",
        "#     model=\"groq-mixtral\",\n",
        "#     messages=messages,\n",
        "# )\n",
        "\n",
        "\n",
        "response = completion(\n",
        "        model=\"groq/mixtral-8x7b-32768\",\n",
        "        messages=messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=200\n",
        "    )"
      ],
      "metadata": {
        "id": "S_p4fQ0pXOyg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(\n",
        "    response.choices[0].message.content\n",
        ")"
      ],
      "metadata": {
        "id": "RqSwY4uQXpbN",
        "outputId": "d76a75bb-b7fc-4540-a4a0-a9134fe5b255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The reason why the sky appears blue during the day is due to a phenomenon '\n",
            " \"called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it is \"\n",
            " 'made up of different colors that are represented in the light spectrum. Each '\n",
            " 'of these colors has a different wavelength, with red having the longest '\n",
            " 'wavelength and violet having the shortest.\\n'\n",
            " '\\n'\n",
            " 'When sunlight enters the atmosphere, it collides with molecules and '\n",
            " 'particles present in the air, such as nitrogen and oxygen. These collisions '\n",
            " 'cause the light to change direction, a process known as scattering. Shorter '\n",
            " 'wavelengths of light, such as violet and blue, are scattered more than '\n",
            " 'longer wavelengths, like red and yellow.\\n'\n",
            " '\\n'\n",
            " 'However, despite violet light being scattered more than blue light, our eyes '\n",
            " 'are more sensitive to blue light and less sensitive to violet light. '\n",
            " 'Additionally, sunlight reaches us with less violet light to begin with due '\n",
            " 'to the absorption and re-emission of light by the ozone')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# response = client.chat.completions.create(\n",
        "#     model=\"groq-mixtral\",\n",
        "#     messages=messages,\n",
        "# )\n",
        "\n",
        "response = completion(\n",
        "        model=\"groq/llama-3.3-70b-versatile\",\n",
        "        messages=messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=200\n",
        "    )\n",
        "pprint(\n",
        "    response.choices[0].message.content\n",
        ")"
      ],
      "metadata": {
        "id": "4t8j1_NPu7vx",
        "outputId": "23570fad-ba99-48f7-e7de-aee3ca75c1e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('السماء تبدو زرقاء في النهار بسبب ظاهرة تسمى تشتت الضوء. عندما يأتي ضوء الشمس '\n",
            " 'إلى الأرض، يتكون من مجموعة من الألوان المختلفة، بما في ذلك الأحمر والبرتقالي '\n",
            " 'والأصفر والأزرق. عندما يمر هذا الضوء خلال الغلاف الجوي للأرض، يصطدم '\n",
            " 'بالجزيئات الصغيرة في الهواء، مثل جزيئات النيتروجين والاوكسجين. هذه الجزيئات '\n",
            " 'الصغيرة تطرد الضوء الأزرق أكثر من الألوان الأخرى لأن طول موجته أقصر. هذا '\n",
            " 'يعني أن الضوء الأزرق ينتشر في جميع الاتجاهات، مما يجعل السماء تبدو زرقاء '\n",
            " 'عندما ننظر إليها.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Balancer"
      ],
      "metadata": {
        "id": "h1-qS5cRX8Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-lb.yaml\n",
        "model_list:\n",
        "  - model_name: \"global-llm\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "      rpm: 20\n",
        "\n",
        "  - model_name: \"global-llm\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "      rpm: 20\n",
        "\n",
        "routing_strategy: simple-shuffle # Literal[\"simple-shuffle\", \"least-busy\",]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO_wS7sfX9oK",
        "outputId": "ad24db56-fe82-43fd-d1ee-ab8e63bbecba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm-lb.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm-lb.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "8eLP5VvDZCmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb773c5-92a5-4782-8794-e089d0f40d42"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "INFO:     Started server process [907]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n",
            "INFO:     Started server process [1234]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8828 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "# from pprint import pprint\n",
        "\n",
        "# client = openai.OpenAI(\n",
        "#     api_key=\"any key\",\n",
        "#     base_url=\"http://0.0.0.0:4000\"\n",
        "# )"
      ],
      "metadata": {
        "id": "E7NlDZOWZL-6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# messages = [\n",
        "#     {\n",
        "#         \"role\": \"user\", # system, assistant\n",
        "#         \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# response = client.chat.completions.create(\n",
        "#     model=\"global-llm\",\n",
        "#     messages=messages,\n",
        "# )\n",
        "\n",
        "# Initialize llmlite client\n",
        "client = litellm.LiteLLM()\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",  # 'system' or 'assistant' roles can also be used\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Send request to the specified model\n",
        "response = client.chat.completions.create(\n",
        "    model=\"groq/llama-3.3-70b-versatile\",  # Use the model name from the YAML file\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R20zh9MqZO-H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the response\n",
        "pprint(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8UyE_Mq3rG7",
        "outputId": "aec81ee8-4c31-476f-da28-fdef68b166b5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('السبب الرئيسي لذلك هو ظاهرة gọi là \"الانتشار الرايلي\" (Rayleigh scattering). '\n",
            " 'عندما يمر الضوء الشمس عبر الغلاف الجوي للأرض، فإنه يصطدم بالجزيئات الصغيرة '\n",
            " 'مثل الغازات والجسيمات في الهواء. ونتيجة لذلك، يتم تشتيت الألوان المختلفة في '\n",
            " 'اتجاهات مختلفة، وذلك بسبب Differences في uzunluk الموجات. \\n'\n",
            " '\\n'\n",
            " 'ألون الضوء الأزرق هي الأقصر وأكثرها انتشارًا من بين الألوان الأخرى، لذلك فهي '\n",
            " 'تتسرب في كل الاتجاهات، مما يجعل السماء تظهر باللون الأزرق. بينما الألوان '\n",
            " 'الأخرى مثل الأحمر والأصفر، لها طول موجي أطول، لذلك لا تنتشر بشكل كبير، '\n",
            " 'وتتحرك في خطوط مستقيمة حتى تصل إلى عيننا، مما يجعلها أقل وضوحًا في السماء.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jdm-hxVkZY_p",
        "outputId": "f676dd75-5931-453e-a9b3-dff61a515525"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'groq/llama-3.3-70b-versatile'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fallbacks"
      ],
      "metadata": {
        "id": "dgVGKuLcapMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "7QM1ZVl-ddv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-fallback.yaml\n",
        "router_settings:\n",
        "  enable_pre_call_checks: true\n",
        "\n",
        "model_list:\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "\n",
        "\n",
        "litellm_settings:\n",
        "  num_retries: 3\n",
        "  fallbacks: [{\"openai-gpt4o-mini\": \"groq-mixtral\"}]\n",
        "  request_timeout: 10\n",
        "  allowed_fails: 3 # per minute\n",
        "  cooldown_time: 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PpSkqgSaqqM",
        "outputId": "10249fef-0012-4b49-9f31-d974562129fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm-fallback.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm-fallback.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "gtEzu5kudiyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4b6c1a-47b4-495a-ef3a-e4a9276fd01d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\n",
            "\n",
            "\u001b[32mLiteLLM: Proxy initialized with Config, Set models:\u001b[0m\n",
            "\u001b[32m    groq-gemma9b\u001b[0m\n",
            "\u001b[32m    groq-mixtral\u001b[0m\n",
            "INFO:     127.0.0.1:50358 - \"POST /chat/completions HTTP/1.1\" 400 Bad Request\n",
            "INFO:     Started server process [5433]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:32663 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observation"
      ],
      "metadata": {
        "id": "70pO-uGvdz0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\""
      ],
      "metadata": {
        "id": "MDmKCOl-d2BV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langfuse==2.52.2"
      ],
      "metadata": {
        "id": "PmRvZY1OgDD0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "collapsed": true,
        "outputId": "52208e66-53d2-4875-ad90-be12ea6f8380"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langfuse==2.52.2\n",
            "  Downloading langfuse-2.52.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting anyio<5.0.0,>=4.4.0 (from langfuse==2.52.2)\n",
            "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langfuse==2.52.2) (2.2.1)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from langfuse==2.52.2) (0.28.1)\n",
            "Requirement already satisfied: idna<4.0,>=3.7 in /usr/local/lib/python3.10/dist-packages (from langfuse==2.52.2) (3.10)\n",
            "Requirement already satisfied: packaging<25.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langfuse==2.52.2) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.10/dist-packages (from langfuse==2.52.2) (2.10.3)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.10/dist-packages (from langfuse==2.52.2) (1.17.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0.0,>=4.4.0->langfuse==2.52.2) (1.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0.0,>=4.4.0->langfuse==2.52.2) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0.0,>=4.4.0->langfuse==2.52.2) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.15.4->langfuse==2.52.2) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.15.4->langfuse==2.52.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==2.52.2) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==2.52.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==2.52.2) (2.27.1)\n",
            "Downloading langfuse-2.52.2-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anyio, langfuse\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyio-4.7.0 langfuse-2.52.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "anyio"
                ]
              },
              "id": "e981d44c991a4a9e9fc3510c009ec6f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-lanfuse.yaml\n",
        "model_list:\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"openai-gpt4o-mini\"\n",
        "    litellm_params:\n",
        "      model: \"openai/gpt-4o-mini\"\n",
        "      api_key: \"os.environ/OPENAI_API_KEY\"\n",
        "\n",
        "litellm_settings:\n",
        "  drop_params: True\n",
        "  success_callback: [\"langfuse\"]\n",
        "  failure_callback: [\"langfuse\"]\n",
        "  redact_user_api_key_info: true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfm6MWZjfQID",
        "outputId": "32189e38-76dc-460f-b814-fbd9cd6685b4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting llm-lanfuse.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm-lanfuse.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "GiaAQkVggT3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7221c62-5507-4a2d-a818-a75d5edd7f15"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\n",
            "\u001b[94m Initialized Success Callbacks - ['langfuse'] \u001b[0m\n",
            "\u001b[94m Initialized Failure Callbacks - ['langfuse'] \u001b[0m\n",
            "\u001b[32mLiteLLM: Proxy initialized with Config, Set models:\u001b[0m\n",
            "\u001b[32m    groq-gemma9b\u001b[0m\n",
            "\u001b[32m    groq-mixtral\u001b[0m\n",
            "INFO:     Started server process [12596]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"OPENAI_API_KEY\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")"
      ],
      "metadata": {
        "id": "vfloPxagilTp"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"openai-gpt4o-mini\",\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "# import openai\n",
        "\n",
        "# # Initialize the OpenAI client with the proxy server's base URL\n",
        "# client =  openai.OpenAI(\n",
        "#     api_key=\"os.environ/GROQ_API_KEY\",  # The actual key is managed by the proxy\n",
        "#     base_url=\"http://0.0.0.0:4000\"  # URL where the litellm proxy is running\n",
        "# )\n",
        "\n",
        "# messages = [\n",
        "#     {\n",
        "#         \"role\": \"user\",\n",
        "#         \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# # Send the request to the proxy server\n",
        "# response = client.chat.completions.create(\n",
        "#     model=\"groq-gemma9b\",\n",
        "#     messages=messages\n",
        "# )\n",
        "\n"
      ],
      "metadata": {
        "id": "D4c-HqL2ioAp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(\n",
        "    response.choices[0].message.content\n",
        ")"
      ],
      "metadata": {
        "id": "K10Sqf87i1TP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ced0a7-d449-4ecc-bc76-87baf357fad3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('يبدو السماء زرقاء خلال النهار بسبب ظاهرة تسمى **散射** الضوء (Rayleigh '\n",
            " 'scattering).\\n'\n",
            " '\\n'\n",
            " 'عندما يدخل الضوء الشمسي الغلاف الجوي للأرض، فإنه يتفاعل مع جزيئات الهواء مثل '\n",
            " 'الأكسجين والهيدروجين. تنتشر هذه الجزيئات الضوء القصير الموجى (الأزرق '\n",
            " 'والأرجواني) في جميع الاتجاهات بشكل أكبر من الضوء الطويل الموجى (الأحمر '\n",
            " 'والبرتقالي). \\n'\n",
            " '\\n'\n",
            " 'هذا يعني أنك ترى المزيد من الضوء الأزرق من خلال جميع جهات السماء، مما يجعلها '\n",
            " 'تبدو زرقاء. \\n'\n",
            " '\\n'\n",
            " '**أسباب أخرى:**\\n'\n",
            " '\\n'\n",
            " '* **زاوية الشمس:** تظهر السماء زرقاء بشكل أكثر وضوحًا عندما تكون الشمس في '\n",
            " 'السماء العليا. عندما تكون الشمس قريبة من الأفق، يمر الضوء عبر كمية أكبر من '\n",
            " 'الغلاف الجوي، فيؤدي ذلك إلى تشتيت لون الأزرق بشكل أكبر ويظهر لون الخافت '\n",
            " 'للأحمر والأصفر.\\n'\n",
            " '* **الضباب:** يمكن أن تؤثر جزيئات الغبرة والضباب على مدى تشتيت الضوء، مما '\n",
            " 'يؤدي إلى إتلاف اللون الأزرق للسماء.\\n'\n",
            " '* **الطقس:** قد يغير السحب والقيمان لون السماء من الأزرق إلى الأبيض أو '\n",
            " 'الداكن.\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " 'أتمنى أن يكون هذا مفيدًا!\\n'\n",
            " '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LiteLLm + LangChain"
      ],
      "metadata": {
        "id": "EiT5j7ZEkST7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain langchain_community transformers"
      ],
      "metadata": {
        "id": "jmhJ-lUFkUf5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACE_API_KEY')\n"
      ],
      "metadata": {
        "id": "05Iy8a1xAonA"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "                            \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "                            \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
        "                            \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
        "                        ])\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "5-xe7jaMlzro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"http://0.0.0.0:4000\",\n",
        "    model = \"openai-gpt4o-mini\",\n",
        "    temperature=0.1\n",
        ")\n",
        "# llm = HuggingFaceHub(\n",
        "#     repo_id=\"Qwen/QwQ-32B-Preview\",  # Replace with the desired model ID\n",
        "#     model_kwargs={\"temperature\": 0.1}\n",
        "# )"
      ],
      "metadata": {
        "id": "oMKDiqA4lMkW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"Write a concise summary of the following:\\n\\n{context}\")]\n",
        ")\n",
        "\n",
        "map_chain = LLMChain(\n",
        "    prompt=map_prompt,\n",
        "    llm=llm,\n",
        "    output_parser=StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "s31-wQu9nB3Z"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = map_chain.invoke({\"context\": docs})"
      ],
      "metadata": {
        "id": "Hyrpm_jAE5Jd"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "IhdlhARDnOCS",
        "outputId": "f004afc6-d22b-4741-bc13-f8ab4b12e2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The document titled \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the concept of building autonomous agents using large language models (LLMs) as their core controllers. It highlights several proof-of-concept demonstrations, such as AutoGPT, GPT-Engineer, and BabyAGI, showcasing LLMs\\' potential as general problem solvers beyond mere text generation. The document outlines the architecture of LLM-powered agents, which includes components for planning, memory, and tool use. \\n\\nKey components include:\\n1. **Planning**: Agents break down tasks into manageable subgoals and reflect on past actions to improve future performance.\\n2. **Memory**: Agents utilize short-term and long-term memory to retain and recall information, often leveraging external vector stores for efficient retrieval.\\n3. **Tool Use**: Agents can call external APIs to access information and perform tasks beyond their pre-trained capabilities.\\n\\nThe document also addresses challenges faced by LLM-powered agents, such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. Overall, it emphasizes the innovative potential of LLMs in creating autonomous agents capable of complex problem-solving and task execution.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=1000,  # Adjust based on your model's token limit\n",
        "#     chunk_overlap=200  # Overlap to maintain context between chunks\n",
        "# )\n",
        "# doc_texts = [doc.page_content for doc in docs]\n",
        "# chunks = text_splitter.split_text(\" \".join(doc_texts))\n"
      ],
      "metadata": {
        "id": "ktgIosmwCpEA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke chain\n",
        "\n",
        "# summaries = []\n",
        "# for chunk in chunks:\n",
        "#     result = map_chain.invoke({\"context\": chunk})\n",
        "#     summaries.append(result)\n",
        "\n",
        "# # Combine summaries into a final summary\n",
        "# final_summary = \" \".join(summaries)\n",
        "# print(final_summary)\n",
        "\n",
        "# print(result)\n",
        "\n",
        "# from langchain.chains import MapReduceDocumentsChain\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=1000,  # Adjust based on your model's token limit\n",
        "#     chunk_overlap=200  # Overlap to maintain context between chunks\n",
        "# )\n",
        "# doc_texts = [doc.page_content for doc in docs]\n",
        "# chunks = text_splitter.split_text(\" \".join(doc_texts))\n",
        "\n",
        "# map_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [(\"system\", \"Write a concise summary of the following:\\n\\n{context}\")]\n",
        "# )\n",
        "# map_reduce_chain = MapReduceDocumentsChain(\n",
        "#     llm_chain=LLMChain(\n",
        "#         prompt=map_prompt,\n",
        "#         llm=llm,\n",
        "#         output_parser=StrOutputParser()\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# map_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
        "# )\n",
        "\n",
        "# map_chain = map_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "igocWVkunKJX"
      },
      "execution_count": 108,
      "outputs": []
    }
  ]
}